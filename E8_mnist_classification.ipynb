{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (E8) Classification of MNIST Hand-written Digits\n",
    "In this exercise, you will be given an example of [MNIST classification](http://yann.lecun.com/exdb/mnist/). \n",
    "You should be able to replicate the results given here if you have completed (E2)-(E5) correctly.\n",
    "\n",
    "It would be best if you have a Python IDE (integrated development environment) such as [PyCharm](https://www.jetbrains.com/pycharm/) and [Anaconda](anaconda.com) is installed because they will make your life easier! If not, you may want to work on the assignment using Google Colab. In any cases, what you need to do is 1) to fill in the blanks in .py files; and 2) to import the files (e.g., layer.py, optim.py, model.py, etc) that you have completed for use. Here are some scenarios how you would go about doing the assignment: \n",
    "\n",
    "#### Without Google Colab: Python IDE + Anaconda \n",
    "If you have a Python IDE and Anaconda installed, you can do one of the following:\n",
    "- Edit .py files in the IDE. Then, simply open .ipynb file also in the IDE where you can edit and run codes. \n",
    "- Your IDE might not support running .ipynb files. However, since you have installed Anaconda, you can just open this notebook using Jupyter Notebook.\n",
    "\n",
    "In both of these cases, you can simply import .py files in this .ipynb file:\n",
    "```python\n",
    "from model import NeuralNetwork\n",
    "```\n",
    " \n",
    "#### With Google Colab\n",
    "- Google Colab has an embedded code editor. So, you could simply upload all .py files to Google Colab and edit the files there. Once you upload the files, double click a file that you want to edit. Please **make sure that you download up-to-date files frequently**, otherwise Google Colab might accidentally restart and all your files might be gone.\n",
    "- If you feel like the above way is cumbersome, you could instead use any online Python editors for completing .py files (e.g., see [repl.it](https://repl.it/languages/python3)). Also, it's not impossible that you edit the files using any text editors, but they don't show you essential Python grammar information, so you'll be prone to make mistakes in that case. Once you are done editing, you can either upload the files to Colab or follow the instruction below. \n",
    " \n",
    "- If you have *git clone*d the assignment repository to a directory in your Google Drive (or you have the files stored in the Drive anyway), you can do the following:\n",
    "```jupyterpython\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')          # this will direct you to a link where you can get an authorization key\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/your-directory-where-the-python-files-exist')\n",
    "```\n",
    "Then, you are good to go. When you change a .py file, make sure it is synced to the drive, then you need to re-run the above lines to get access to the latest version of the file. Note that you should give correct path to *sys.path.append* method.\n",
    "\n",
    "Now, let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset\n",
    "MNIST dataset has been one of the most frequently used dataset. Among the total of 70,000 (28x28) images, 60,000 are used for training, while 10,000 are reserved for testing. The images have only 1 channel (hence, black and white), and each pixel has a value between 0 to 255 (integers). The labels are also integers which indicate the number written in the corresponding images. Often, the class labels are one-hot encoded during preprocessing.\n",
    "\n",
    "Some simple preprocessing like below is normally done on the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "np.random.seed(100)                         # fix a random seed for reproducibility\n",
    "\n",
    "# download the dataset (this will take some time)\n",
    "mnist = fetch_openml('mnist_784', cache=False)\n",
    "num_train = 60000\n",
    "image = mnist.data\n",
    "label = mnist.target.astype('int64')\n",
    "\n",
    "# normalize pixel values to (-0.5, 0.5) range\n",
    "image = image / 255 - 0.5\n",
    "\n",
    "# train test split\n",
    "train_image, train_label, test_image, test_label = \\\n",
    "        image[:num_train], label[:num_train], image[num_train:], label[num_train:]\n",
    "\n",
    "# One-hot encoding\n",
    "train_label, test_label = np.eye(10)[train_label], np.eye(10)[test_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "Let's define a linear neural network model which has no hidden layers. Since we are solving a classification problem, we need to use the softmax output and the cross entropy loss. Note that this reduces to the logistic regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# import files\n",
    "from model import NeuralNetwork\n",
    "from layer import FCLayer\n",
    "from loss import CrossEntropyLoss\n",
    "from optim import SGD, Adam, RMSProp\n",
    "from utils import *\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "nn.add(FCLayer(train_image.shape[1], train_label.shape[1], initialization='xavier', uniform=True)) # no hidden layers.direct mapping from input images to target labels\n",
    "\n",
    "print(train_image.shape[1])\n",
    "print(train_label.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set loss and link to the model\n",
    "loss = CrossEntropyLoss()\n",
    "nn.set_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set hyperparamters\n",
    "lr = 0.001                                  # learning rate\n",
    "batch_size = 32                             # mini-batch size\n",
    "epochs = 5                                  # number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# set optimizer and link to the model\n",
    "optimizer = Adam(nn.parameters(), lr=lr)\n",
    "nn.set_optimizer(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\terror=0.48863\tTest accuracy: 0.8972\n",
      "Epoch 2/5\terror=0.33068\tTest accuracy: 0.9128\n",
      "Epoch 3/5\terror=0.31043\tTest accuracy: 0.9146\n",
      "Epoch 4/5\terror=0.29940\tTest accuracy: 0.9168\n",
      "Epoch 5/5\terror=0.29452\tTest accuracy: 0.9151"
     ]
    }
   ],
   "source": [
    "inds = list(range(train_image.shape[0]))\n",
    "N = train_image.shape[0]                               # number of training samples\n",
    "\n",
    "loss_hist = []\n",
    "for epoch in range(epochs):\n",
    "    # randomly shuffle the training data at the beginning of each epoch\n",
    "    inds = np.random.permutation(inds)\n",
    "    x_train = train_image[inds]\n",
    "    y_train = train_label[inds]\n",
    "\n",
    "    loss = 0\n",
    "    for b in range(0, N, batch_size):\n",
    "        # get the mini-batch\n",
    "        x_batch = x_train[b: b + batch_size]\n",
    "        y_batch = y_train[b: b + batch_size]\n",
    "\n",
    "        # feed forward\n",
    "        pred = nn.predict(x_batch)\n",
    "\n",
    "        # Error\n",
    "        loss += nn.loss(pred, y_batch) / N\n",
    "\n",
    "        # Back propagation of errors\n",
    "        nn.backward(pred, y_batch)\n",
    "\n",
    "        # Update parameters\n",
    "        nn.optimizer.step()\n",
    "\n",
    "    # record loss per epoch\n",
    "    loss_hist.append(loss)\n",
    "\n",
    "    print()\n",
    "    print(\"Epoch %d/%d\\terror=%.5f\" % (epoch + 1, epochs, loss), end='\\t', flush=True)\n",
    "\n",
    "    # Test accuracy\n",
    "    pred = softmax(nn.predict(test_image, mode=False))\n",
    "    y_pred, y_target = np.argmax(pred, axis=1), np.argmax(test_label, axis=1)\n",
    "    accuracy = np.mean(y_pred == y_target)\n",
    "    print(\"Test accuracy: {:.4f}\".format(accuracy), end='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## (E8) Your Turn: Non-linear Neural Network\n",
    "Surprisingly, the model achieved more than 91% test accuracy. However, you can definitely improve the test performance by, for example, introducing nonlinear activation functions, changing the network architecture, adjusting the learning rate, training more epochs, and (or) using a different optimizer. **It's your turn to try different configurations of these!** \n",
    "\n",
    "*Experiment with more than 3 configurations of these to get better test performance, and report your trials by summarizing the configurations and performance in a **table**. (You can achieve *at least* 96% accuracy pretty easily.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\terror=0.30455\tTest accuracy: 0.9559\n",
      "Epoch 2/5\terror=0.14983\tTest accuracy: 0.9602\n",
      "Epoch 3/5\terror=0.12044\tTest accuracy: 0.9667\n",
      "Epoch 4/5\terror=0.10602\tTest accuracy: 0.9677\n",
      "Epoch 5/5\terror=0.09499\tTest accuracy: 0.9702"
     ]
    }
   ],
   "source": [
    "#Versions 1: Adding a hidden layer, using ReLU activation, RMSprop optimizer\n",
    "# import files\n",
    "from model import NeuralNetwork\n",
    "from layer import FCLayer\n",
    "from loss import CrossEntropyLoss\n",
    "from optim import SGD, Adam, RMSProp\n",
    "from utils import *\n",
    "from activation import Activation\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "nn.add(FCLayer(train_image.shape[1], train_image.shape[1], initialization='xavier', uniform=True))  # no hidden layers. direct mapping from input images to target labels\n",
    "nn.add(Activation(relu, relu_prime))\n",
    "nn.add(FCLayer(train_image.shape[1], train_label.shape[1], initialization='xavier', uniform=True))\n",
    "# Set loss and link to the model\n",
    "\n",
    "\n",
    "loss = CrossEntropyLoss()\n",
    "nn.set_loss(loss)\n",
    "\n",
    "# Set hyperparamters\n",
    "lr = 0.001                                  # learning rate\n",
    "batch_size = 32                             # mini-batch size\n",
    "epochs = 5                                  # number of epochs\n",
    "\n",
    "# set optimizer and link to the model\n",
    "optimizer = RMSProp(nn.parameters(),lr= 0.001, beta_1=0.9, beta_2=0.999)\n",
    "nn.set_optimizer(optimizer)\n",
    "\n",
    "inds = list(range(train_image.shape[0]))\n",
    "N = train_image.shape[0]                               # number of training samples\n",
    "\n",
    "loss_hist = []\n",
    "for epoch in range(epochs):\n",
    "    # randomly shuffle the training data at the beginning of each epoch\n",
    "    inds = np.random.permutation(inds)\n",
    "    x_train = train_image[inds]\n",
    "    y_train = train_label[inds]\n",
    "\n",
    "    loss = 0\n",
    "    for b in range(0, N, batch_size):\n",
    "        # get the mini-batch\n",
    "        x_batch = x_train[b: b + batch_size]\n",
    "        y_batch = y_train[b: b + batch_size]\n",
    "\n",
    "        # feed forward\n",
    "        pred = nn.predict(x_batch)\n",
    "\n",
    "        # Error\n",
    "        loss += nn.loss(pred, y_batch) / N\n",
    "\n",
    "        # Back propagation of errors\n",
    "        nn.backward(pred, y_batch)\n",
    "\n",
    "        # Update parameters\n",
    "        nn.optimizer.step()\n",
    "\n",
    "    # record loss per epoch\n",
    "    loss_hist.append(loss)\n",
    "\n",
    "    print()\n",
    "    print(\"Epoch %d/%d\\terror=%.5f\" % (epoch + 1, epochs, loss), end='\\t', flush=True)\n",
    "\n",
    "    # Test accuracy\n",
    "    pred = softmax(nn.predict(test_image, mode=False))\n",
    "    y_pred, y_target = np.argmax(pred, axis=1), np.argmax(test_label, axis=1)\n",
    "    accuracy = np.mean(y_pred == y_target)\n",
    "    print(\"Test accuracy: {:.4f}\".format(accuracy), end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\terror=0.37052\tTest accuracy: 0.9228\n",
      "Epoch 2/10\terror=0.18464\tTest accuracy: 0.9504\n",
      "Epoch 3/10\terror=0.12124\tTest accuracy: 0.9656\n",
      "Epoch 4/10\terror=0.09089\tTest accuracy: 0.9692\n",
      "Epoch 5/10\terror=0.07194\tTest accuracy: 0.9757\n",
      "Epoch 6/10\terror=0.05812\tTest accuracy: 0.9749\n",
      "Epoch 7/10\terror=0.04684\tTest accuracy: 0.9774\n",
      "Epoch 8/10\terror=0.04125\tTest accuracy: 0.9788\n",
      "Epoch 9/10\terror=0.03402\tTest accuracy: 0.9797\n",
      "Epoch 10/10\terror=0.03154\tTest accuracy: 0.9760"
     ]
    }
   ],
   "source": [
    "#Variation 2: Adding a hidden layer, using sigmoid activation, using 10 epochs, and using adam optimizer\n",
    "# import files\n",
    "from model import NeuralNetwork\n",
    "from layer import FCLayer\n",
    "from loss import CrossEntropyLoss\n",
    "from optim import SGD, Adam, RMSProp\n",
    "from utils import *\n",
    "from activation import Activation\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "nn.add(FCLayer(train_image.shape[1], train_image.shape[1], initialization='xavier', uniform=True))  # no hidden layers. direct mapping from input images to target labels\n",
    "nn.add(Activation(sigmoid, sigmoid_prime))\n",
    "nn.add(FCLayer(train_image.shape[1], train_label.shape[1], initialization='xavier', uniform=True))\n",
    "# Set loss and link to the model\n",
    "\n",
    "\n",
    "\n",
    "loss = CrossEntropyLoss()\n",
    "nn.set_loss(loss)\n",
    "\n",
    "# Set hyperparamters\n",
    "lr = 0.001                                  # learning rate\n",
    "batch_size = 32                             # mini-batch size\n",
    "epochs = 10                                  # number of epochs\n",
    "\n",
    "# set optimizer and link to the model\n",
    "optimizer = Adam(nn.parameters(), lr=lr)\n",
    "nn.set_optimizer(optimizer)\n",
    "\n",
    "inds = list(range(train_image.shape[0]))\n",
    "N = train_image.shape[0]                               # number of training samples\n",
    "\n",
    "loss_hist = []\n",
    "for epoch in range(epochs):\n",
    "    # randomly shuffle the training data at the beginning of each epoch\n",
    "    inds = np.random.permutation(inds)\n",
    "    x_train = train_image[inds]\n",
    "    y_train = train_label[inds]\n",
    "\n",
    "    loss = 0\n",
    "    for b in range(0, N, batch_size):\n",
    "        # get the mini-batch\n",
    "        x_batch = x_train[b: b + batch_size]\n",
    "        y_batch = y_train[b: b + batch_size]\n",
    "\n",
    "        # feed forward\n",
    "        pred = nn.predict(x_batch)\n",
    "\n",
    "        # Error\n",
    "        loss += nn.loss(pred, y_batch) / N\n",
    "\n",
    "        # Back propagation of errors\n",
    "        nn.backward(pred, y_batch)\n",
    "\n",
    "        # Update parameters\n",
    "        nn.optimizer.step()\n",
    "\n",
    "    # record loss per epoch\n",
    "    loss_hist.append(loss)\n",
    "\n",
    "    print()\n",
    "    print(\"Epoch %d/%d\\terror=%.5f\" % (epoch + 1, epochs, loss), end='\\t', flush=True)\n",
    "\n",
    "    # Test accuracy\n",
    "    pred = softmax(nn.predict(test_image, mode=False))\n",
    "    y_pred, y_target = np.argmax(pred, axis=1), np.argmax(test_label, axis=1)\n",
    "    accuracy = np.mean(y_pred == y_target)\n",
    "    print(\"Test accuracy: {:.4f}\".format(accuracy), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\terror=0.27673\tTest accuracy: 0.9599\n",
      "Epoch 2/10\terror=0.13215\tTest accuracy: 0.9545\n",
      "Epoch 3/10\terror=0.10089\tTest accuracy: 0.9664\n",
      "Epoch 4/10\terror=0.08067\tTest accuracy: 0.9716\n",
      "Epoch 5/10\terror=0.06761\tTest accuracy: 0.9736\n",
      "Epoch 6/10\terror=0.05998\tTest accuracy: 0.9715\n",
      "Epoch 7/10\terror=0.05429\tTest accuracy: 0.9726\n",
      "Epoch 8/10\terror=0.04536\tTest accuracy: 0.9755\n",
      "Epoch 9/10\terror=0.04100\tTest accuracy: 0.9748\n",
      "Epoch 10/10\terror=0.03823\tTest accuracy: 0.9729"
     ]
    }
   ],
   "source": [
    "#Variation 3: Adding a hidden layer and using relu activation and adam optimizer\n",
    "# import files\n",
    "from model import NeuralNetwork\n",
    "from layer import FCLayer\n",
    "from loss import CrossEntropyLoss\n",
    "from optim import SGD, Adam, RMSProp\n",
    "from utils import *\n",
    "from activation import Activation\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "nn.add(FCLayer(train_image.shape[1], train_image.shape[1], initialization='xavier', uniform=True))  \n",
    "nn.add(Activation(relu, relu_prime))\n",
    "nn.add(FCLayer(train_image.shape[1], train_label.shape[1], initialization='xavier', uniform=True))\n",
    "# Set loss and link to the model\n",
    "\n",
    "loss = CrossEntropyLoss()\n",
    "nn.set_loss(loss)\n",
    "\n",
    "# Set hyperparamters\n",
    "lr = 0.001                                  # learning rate\n",
    "batch_size = 32                             # mini-batch size\n",
    "epochs = 10                                # number of epochs\n",
    "\n",
    "# set optimizer and link to the model\n",
    "optimizer = Adam(nn.parameters(), lr=lr)\n",
    "nn.set_optimizer(optimizer)\n",
    "\n",
    "inds = list(range(train_image.shape[0]))\n",
    "N = train_image.shape[0]                               # number of training samples\n",
    "\n",
    "loss_hist = []\n",
    "for epoch in range(epochs):\n",
    "    # randomly shuffle the training data at the beginning of each epoch\n",
    "    inds = np.random.permutation(inds)\n",
    "    x_train = train_image[inds]\n",
    "    y_train = train_label[inds]\n",
    "\n",
    "    loss = 0\n",
    "    for b in range(0, N, batch_size):\n",
    "        # get the mini-batch\n",
    "        x_batch = x_train[b: b + batch_size]\n",
    "        y_batch = y_train[b: b + batch_size]\n",
    "\n",
    "        # feed forward\n",
    "        pred = nn.predict(x_batch)\n",
    "\n",
    "        # Error\n",
    "        loss += nn.loss(pred, y_batch) / N\n",
    "\n",
    "        # Back propagation of errors\n",
    "        nn.backward(pred, y_batch)\n",
    "\n",
    "        # Update parameters\n",
    "        nn.optimizer.step()\n",
    "\n",
    "    # record loss per epoch\n",
    "    loss_hist.append(loss)\n",
    "\n",
    "    print()\n",
    "    print(\"Epoch %d/%d\\terror=%.5f\" % (epoch + 1, epochs, loss), end='\\t', flush=True)\n",
    "\n",
    "    # Test accuracy\n",
    "    pred = softmax(nn.predict(test_image, mode=False))\n",
    "    y_pred, y_target = np.argmax(pred, axis=1), np.argmax(test_label, axis=1)\n",
    "    accuracy = np.mean(y_pred == y_target)\n",
    "    print(\"Test accuracy: {:.4f}\".format(accuracy), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\terror=0.31502\tTest accuracy: 0.9544\n",
      "Epoch 2/5\terror=0.15421\tTest accuracy: 0.9493\n",
      "Epoch 3/5\terror=0.11674\tTest accuracy: 0.9605\n",
      "Epoch 4/5\terror=0.09475\tTest accuracy: 0.9700\n",
      "Epoch 5/5\terror=0.08375\tTest accuracy: 0.9676"
     ]
    }
   ],
   "source": [
    "#Variation 4: Adding a hidden layer, using tanh activation and using Adam optimizer\n",
    "# import files\n",
    "from model import NeuralNetwork\n",
    "from layer import FCLayer\n",
    "from loss import CrossEntropyLoss\n",
    "from optim import SGD, Adam, RMSProp\n",
    "from utils import *\n",
    "from activation import Activation\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "nn.add(FCLayer(train_image.shape[1], train_image.shape[1], initialization='xavier', uniform=True))# add hidden layer\n",
    "nn.add(Activation(tanh, tanh_prime))\n",
    "nn.add(FCLayer(train_image.shape[1], train_label.shape[1], initialization='xavier', uniform=True))\n",
    "# Set loss and link to the model\n",
    "\n",
    "loss = CrossEntropyLoss()\n",
    "nn.set_loss(loss)\n",
    "\n",
    "# Set hyperparamters\n",
    "lr = 0.001                                  # learning rate\n",
    "batch_size = 32                             # mini-batch size\n",
    "epochs = 5                                  # number of epochs\n",
    "\n",
    "# set optimizer and link to the model\n",
    "optimizer = Adam(nn.parameters(), lr=lr)\n",
    "nn.set_optimizer(optimizer)\n",
    "\n",
    "inds = list(range(train_image.shape[0]))\n",
    "N = train_image.shape[0]                               # number of training samples\n",
    "\n",
    "loss_hist = []\n",
    "for epoch in range(epochs):\n",
    "    # randomly shuffle the training data at the beginning of each epoch\n",
    "    inds = np.random.permutation(inds)\n",
    "    x_train = train_image[inds]\n",
    "    y_train = train_label[inds]\n",
    "\n",
    "    loss = 0\n",
    "    for b in range(0, N, batch_size):\n",
    "        # get the mini-batch\n",
    "        x_batch = x_train[b: b + batch_size]\n",
    "        y_batch = y_train[b: b + batch_size]\n",
    "\n",
    "        # feed forward\n",
    "        pred = nn.predict(x_batch)\n",
    "\n",
    "        # Error\n",
    "        loss += nn.loss(pred, y_batch) / N\n",
    "\n",
    "        # Back propagation of errors\n",
    "        nn.backward(pred, y_batch)\n",
    "\n",
    "        # Update parameters\n",
    "        nn.optimizer.step()\n",
    "\n",
    "    # record loss per epoch\n",
    "    loss_hist.append(loss)\n",
    "\n",
    "    print()\n",
    "    print(\"Epoch %d/%d\\terror=%.5f\" % (epoch + 1, epochs, loss), end='\\t', flush=True)\n",
    "\n",
    "    # Test accuracy\n",
    "    pred = softmax(nn.predict(test_image, mode=False))\n",
    "    y_pred, y_target = np.argmax(pred, axis=1), np.argmax(test_label, axis=1)\n",
    "    accuracy = np.mean(y_pred == y_target)\n",
    "    print(\"Test accuracy: {:.4f}\".format(accuracy), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note the tables for E7 and E8 are in a separate PDF file. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
