{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (E6) Autoencoders\n",
    "In this exercise, you will be given an example of [autoencoders](https://en.wikipedia.org/wiki/Autoencoder). \n",
    "You should be able to replicate the results given here if you have completed (E2)-(E5) correctly.\n",
    "\n",
    "It would be best if you have a Python IDE (integrated development environment) such as [PyCharm](https://www.jetbrains.com/pycharm/) and [Anaconda](anaconda.com) is installed because they will make your life easier! If not, you may want to work on the assignment using Google Colab. In any cases, what you need to do is 1) to fill in the blanks in .py files; and 2) to import the files (e.g., layer.py, optim.py, model.py, etc) that you have completed for use. Here are some scenarios how you would go about doing the assignment: \n",
    "\n",
    "#### Without Google Colab: Python IDE + Anaconda \n",
    "If you have a Python IDE and Anaconda installed, you can do one of the following:\n",
    "- Edit .py files in the IDE. Then, simply open .ipynb file also in the IDE where you can edit and run codes. \n",
    "- Your IDE might not support running .ipynb files. However, since you have installed Anaconda, you can just open this notebook using Jupyter Notebook.\n",
    "\n",
    "In both of these cases, you can simply import .py files in this .ipynb file:\n",
    "```python\n",
    "from model import NeuralNetwork\n",
    "```\n",
    " \n",
    "#### With Google Colab\n",
    "- Google Colab has an embedded code editor. So, you could simply upload all .py files to Google Colab and edit the files there. Once you upload the files, double click a file that you want to edit. Please **make sure that you download up-to-date files frequently**, otherwise Google Colab might accidentally restart and all your files might be gone.\n",
    "- If you feel like the above way is cumbersome, you could instead use any online Python editors for completing .py files (e.g., see [repl.it](https://repl.it/languages/python3)). Also, it's not impossible that you edit the files using any text editors, but they don't show you essential Python grammar information, so you'll be prone to make mistakes in that case. Once you are done editing, you can either upload the files to Colab or follow the instruction below. \n",
    " \n",
    "- If you have *git clone*d the assignment repository to a directory in your Google Drive (or you have the files stored in the Drive anyway), you can do the following:\n",
    "```jupyterpython\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')          # this will direct you to a link where you can get an authorization key\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/your-directory-where-the-python-files-exist')\n",
    "```\n",
    "Then, you are good to go. When you change a .py file, make sure it is synced to the drive, then you need to re-run the above lines to get access to the latest version of the file. Note that you should give correct path to *sys.path.append* method.\n",
    "\n",
    "Now, let's get started!\n",
    "## Autoencoder\n",
    "### Input and Target\n",
    "An autoencoder learns the latent embeddings of inputs in an unsupervised way. This is because we do not need to have specific target values associated with the inputs; however, the input data themselves will act as the targets. \n",
    "\n",
    "To see it more concretely, let's look at below code which prepares the data for learning an autoencoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def generate_data(num=8):\n",
    "    \"\"\" Generate 'num' number of one-hot encoded integers. \"\"\" \n",
    "    x_train = np.eye(num)[np.arange(num)]                       # This is a simple way to one-hot encode integers\n",
    "    \n",
    "    # Repeat x_train multiple times for training\n",
    "    x_train = np.repeat(x_train, 100, axis=0)\n",
    "    \n",
    "    # The target is x_train itself!\n",
    "    x_target = x_train.copy()\n",
    "    return x_train, x_target    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Clearly, *x_target* is the same as *x_train*. So, what we want to do is to encode 8-bit inputs using 3 hidden nodes, which in turn will be decoded back to the original 8-bit value by the decoder. Learning an autoencoder, therefore, means that we train both the encoder weight and the decoder weight. In our example, since we have 3 hidden nodes in a single layer, the encoder weight has *[8, 3]* shape, whereas the decoder weight has *[3, 8]* shape. \n",
    "\n",
    "### Training an Autoencoder\n",
    "Now, let us train an autoencoder with the sigmoid activation function and the cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from model import NeuralNetwork\n",
    "from layer import FCLayer\n",
    "from activation import Activation\n",
    "from utils import *\n",
    "from loss import CrossEntropyLoss\n",
    "from optim import SGD, Adam, RMSProp\n",
    "# Load data\n",
    "num = 8\n",
    "np.random.seed(10)\n",
    "x_train, x_target = generate_data(num=num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define a model and add fully-connected and activation layers.\n",
    "nn = NeuralNetwork()\n",
    "nn.add(FCLayer(x_train.shape[1], 3, initialization='xavier', uniform=False))\n",
    "nn.add(Activation(sigmoid, sigmoid_prime))\n",
    "nn.add(FCLayer(3, x_train.shape[1], initialization='xavier', uniform=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define loss: note that CrossEntropyLoss is using the softmax output internally\n",
    "loss = CrossEntropyLoss()\n",
    "nn.set_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set up hyperparameters\n",
    "lr = 0.001\n",
    "epochs = 2000\n",
    "freq = epochs // 10\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define optimizer and associate it with the model\n",
    "optimizer = Adam(nn.parameters(), lr=lr)\n",
    "nn.set_optimizer(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/2000\tloss=0.00008\tTest loss: 0.06763\n",
      "Epoch 201/2000\tloss=0.00003\tTest loss: 0.02636\n",
      "Epoch 401/2000\tloss=0.00001\tTest loss: 0.01029\n",
      "Epoch 601/2000\tloss=0.00001\tTest loss: 0.00403\n",
      "Epoch 801/2000\tloss=0.00000\tTest loss: 0.00158\n",
      "Epoch 1001/2000\tloss=0.00000\tTest loss: 0.00062\n",
      "Epoch 1201/2000\tloss=0.00000\tTest loss: 0.00024\n",
      "Epoch 1401/2000\tloss=0.00000\tTest loss: 0.00010\n",
      "Epoch 1601/2000\tloss=0.00000\tTest loss: 0.00004\n",
      "Epoch 1801/2000\tloss=0.00000\tTest loss: 0.00002\n",
      "Training finished!\n",
      "Print prediction results:\n",
      "\tInput: [1. 0. 0. 0. 0. 0. 0. 0.]\tOutput: [[1.00e+00 1.20e-27 3.28e-09 3.28e-21 3.01e-10 1.77e-15 6.22e-21 2.54e-09]]\n",
      "\tInput: [0. 1. 0. 0. 0. 0. 0. 0.]\tOutput: [[1.08e-29 1.00e+00 3.54e-19 1.59e-09 1.05e-16 6.43e-09 1.59e-09 4.53e-19]]\n",
      "\tInput: [0. 0. 1. 0. 0. 0. 0. 0.]\tOutput: [[2.02e-09 3.16e-19 1.00e+00 3.50e-30 5.38e-17 6.24e-09 1.77e-09 3.06e-19]]\n",
      "\tInput: [0. 0. 0. 1. 0. 0. 0. 0.]\tOutput: [[3.60e-21 2.54e-09 7.80e-28 1.00e+00 3.93e-10 1.22e-15 3.76e-21 2.53e-09]]\n",
      "\tInput: [0. 0. 0. 0. 1. 0. 0. 0.]\tOutput: [[1.89e-09 5.58e-15 6.23e-15 1.57e-09 1.00e+00 2.73e-19 2.07e-09 5.25e-15]]\n",
      "\tInput: [0. 0. 0. 0. 0. 1. 0. 0.]\tOutput: [[1.38e-16 5.13e-09 4.48e-09 7.93e-17 7.54e-22 1.00e+00 1.15e-16 5.27e-09]]\n",
      "\tInput: [0. 0. 0. 0. 0. 0. 1. 0.]\tOutput: [[6.46e-21 2.50e-09 3.22e-09 2.85e-21 3.04e-10 1.64e-15 1.00e+00 1.07e-27]]\n",
      "\tInput: [0. 0. 0. 0. 0. 0. 0. 1.]\tOutput: [[1.56e-09 4.44e-19 3.35e-19 1.70e-09 9.64e-17 6.47e-09 9.22e-30 1.00e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Training begins\n",
    "inds = list(range(x_train.shape[0]))\n",
    "N = x_train.shape[0]\n",
    "\n",
    "loss_hist = []\n",
    "for epoch in range(epochs):\n",
    "    inds = np.random.permutation(inds)\n",
    "    x_train = x_train[inds]\n",
    "    x_target = x_target[inds]\n",
    "    \n",
    "    loss = 0\n",
    "    for b in range(0, N, batch_size):\n",
    "        # get the mini-batch\n",
    "        x_batch = x_train[b: b+batch_size]\n",
    "        x_target_batch = x_target[b: b+batch_size]\n",
    "        #print(x_batch)\n",
    "        \n",
    "        # feed forward\n",
    "        pred = nn.predict(x_batch)\n",
    "        \n",
    "        # Error\n",
    "        loss += nn.loss(pred, x_target_batch) / N\n",
    "        \n",
    "        # Back propagation of error\n",
    "        nn.backward(pred, x_target_batch)\n",
    "        \n",
    "        # Update parameters\n",
    "        nn.optimizer.step()\n",
    "\n",
    "    # Record loss per epoch\n",
    "    loss_hist.append(loss)\n",
    "\n",
    "    if epoch % freq == 0:\n",
    "        print()\n",
    "        print(\"Epoch %d/%d\\tloss=%.5f\" % (epoch + 1, epochs, loss), end='\\t', flush=True)\n",
    "        \n",
    "        # Test with the training data\n",
    "        pred = nn.predict(x_train, mode=False)\n",
    "        l = nn.loss(pred, x_target)\n",
    "        print(\"Test loss: {:.5f}\".format(l), end='')\n",
    "\n",
    "print(\"\\nTraining finished!\")\n",
    "print(\"Print prediction results:\")\n",
    "x_test = np.eye(num)[np.arange(num)]                        # Test data (one-hot encoded)\n",
    "np.set_printoptions(2)\n",
    "for x in x_test:\n",
    "    print(\"\\tInput: {}\\tOutput: {}\".format(x, softmax(nn.predict(x[None, :], mode=False))))\n",
    "    print(\"Pass activation:\", nn.layers[2].input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If you look at the output values of the network, clearly we have successfully trained the autoencoder to encode-decode 8-bit integers!\n",
    "\n",
    "## (E7) Your Turn:  Explain the autoencoder\n",
    "Given the trained model that can encode the 0-7 integers, explain how the NN model learned to encode/compress the numbers. Rather than just stating your reasoning in words, do explore the model closely to see what it has learned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autoencoder works in two parts: the encoder and the decoder. First, the encoder take the 8x8 matrix and converts it into an 8x3 matrix. The decoder, then takes the 8x3 matrix and attempts to decode it and approximate it as best as possible into the original 8x8 matrix. I've printed out the values for the final weights and biases for the network below and all the activations for each pass above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [[-10.68   8.31 -10.12]\n",
      " [ 10.31 -12.06   9.9 ]\n",
      " [ 10.4    8.04 -10.74]\n",
      " [-10.43 -11.29   9.79]\n",
      " [-10.4  -10.86  -9.89]\n",
      " [ 11.15   9.21  10.29]\n",
      " [  9.88 -11.12 -10.11]\n",
      " [-11.15   9.    10.04]]\n",
      "Bias: [[-0.07  1.21 -0.04]]\n"
     ]
    }
   ],
   "source": [
    "#Weights & bias\n",
    "print(\"Weights:\",nn.layers[0].weights.value)\n",
    "print(\"Bias:\",nn.layers[0].bias.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-10.74   9.52 -10.16]\n",
      " [ 10.24 -10.86   9.87]\n",
      " [ 10.33   9.25 -10.78]\n",
      " [-10.49 -10.08   9.75]\n",
      " [-10.47  -9.66  -9.93]\n",
      " [ 11.09  10.41  10.25]\n",
      " [  9.81  -9.91 -10.14]\n",
      " [-11.22  10.21  10.  ]]\n"
     ]
    }
   ],
   "source": [
    "#The following is the encoded x_test matrix into an 8x3 matrix\n",
    "print(x_test @ nn.layers[0].weights.value + nn.layers[0].bias.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#This takes the above matrix and puts it through the forward method, this is the activation \n",
    "print(np.round(nn.layers[1].forward(x_test @ nn.layers[0].weights.value + nn.layers[0].bias.value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 25.47 -36.52   5.93 -21.7    3.54  -8.5  -21.06   5.68]]\n",
      "[[-46.78  19.92 -22.57  -0.35 -16.88   1.05  -0.34 -22.32]]\n",
      "[[ -0.13 -22.71  19.89 -47.93 -17.57   1.    -0.26 -22.74]]\n",
      "[[-21.19   6.1  -36.53  25.89   4.23  -8.45 -21.14   6.09]]\n",
      "[[  4.93  -7.8   -7.69   4.75  25.02 -17.72   5.03  -7.86]]\n",
      "[[-26.25  -8.81  -8.95 -26.8  -38.36  10.27 -26.43  -8.79]]\n",
      "[[-20.66   6.02   6.27 -21.48   3.91  -8.22  25.83 -36.27]]\n",
      "[[ -0.65 -22.63 -22.91  -0.57 -17.25   0.77 -47.23  19.63]]\n"
     ]
    }
   ],
   "source": [
    "#forward pass for data\n",
    "for x in x_test:\n",
    "    print(nn.predict(x[None, :], mode=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    " for x in x_test:\n",
    "    print(np.round(softmax(nn.predict(x[None, :], mode=False)))) #after rounding we can see that the output is the same as the x_Test input"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
